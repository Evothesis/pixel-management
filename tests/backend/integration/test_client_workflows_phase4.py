"""
Phase 4: Comprehensive Client Workflow Integration Test Suite.

This module validates complete client lifecycle workflows with enterprise-grade testing
from creation through deactivation, including domain management, configuration updates,
and cross-client operations. Tests ensure 90%+ coverage of data consistency across all
system components and proper error handling in complex multi-step operations.

Phase 4 Test Categories:
- Complete client setup flow from creation to domain assignment with validation
- Client domain addition flow with configuration impact and conflict resolution
- Client configuration update flow with rollback and validation testing
- Client deactivation flow with comprehensive cleanup and audit trail
- Cross-client domain conflict resolution with security and performance testing

All tests validate end-to-end data consistency, proper error handling,
rollback capabilities, business rule enforcement, and performance requirements
across the entire system. Coverage target: ≥90% for integration workflows.
"""

import pytest
import asyncio
from datetime import datetime, timedelta
from unittest.mock import patch, MagicMock
from httpx import AsyncClient
import time
import json
from typing import Dict, Any, List

from app.main import app
from app.schemas import ClientCreate, ClientUpdate, DomainCreate


class TestClientWorkflowsPhase4:
    """Phase 4: Enterprise-grade test suite for end-to-end client workflow validation."""

    @pytest.mark.asyncio
    async def test_complete_client_setup_flow(self, test_client, mock_firestore_client, client_factory, domain_factory):
        """
        Phase 4: Test complete client setup from creation to domain assignment with comprehensive validation.
        
        Validates:
        - Client creation with full validation and business rule enforcement
        - Domain addition and authorization with security verification
        - Configuration validation and application with performance monitoring
        - Data consistency across all collections with integrity checks
        - Proper indexing and lookup capabilities with optimization
        - Error handling and rollback capabilities with audit trail
        - Performance requirements and scalability validation
        """
        # Step 1: Create client with comprehensive validation
        client_data = {
            'name': 'Complete Setup Enterprise Company',
            'email': 'setup@enterprise.com',
            'owner': 'owner@enterprise.com',
            'billing_entity': 'billing@enterprise.com',
            'client_type': 'enterprise',
            'deployment_type': 'dedicated',
            'privacy_level': 'gdpr',
            'features': {
                'analytics': {
                    'enabled': True,
                    'tracking_level': 'enhanced',
                    'retention_days': 1095,
                    'cross_domain': True,
                    'custom_dimensions': ['category', 'user_segment', 'campaign_source'],
                    'real_time': {
                        'enabled': True,
                        'refresh_interval_seconds': 30,
                        'sample_rate': 0.1
                    }
                },
                'conversion_tracking': {
                    'enabled': True,
                    'attribution_window_days': 30,
                    'cross_device': True,
                    'enhanced_conversions': True,
                    'goals': [
                        {
                            'name': 'purchase',
                            'value_threshold': 100.0,
                            'currency': 'USD',
                            'funnel_optimization': True
                        },
                        {
                            'name': 'lead_generation',
                            'value_threshold': 0.0,
                            'currency': 'USD',
                            'qualification_score': 'high'
                        }
                    ]
                },
                'privacy_compliance': {
                    'gdpr_enabled': True,
                    'ccpa_enabled': True,
                    'consent_management': {
                        'framework': 'IAB_TCF_v2',
                        'auto_detection': True,
                        'explicit_consent': True
                    },
                    'data_processing': {
                        'pseudonymization': True,
                        'encryption_at_rest': True,
                        'right_to_be_forgotten': True
                    }
                },
                'performance_optimization': {
                    'cdn_enabled': True,
                    'edge_caching': True,
                    'compression': 'brotli',
                    'minification': True,
                    'lazy_loading': True
                }\n            }\n        }\n        \n        # Performance monitoring for client creation\n        start_time = time.time()\n        response = await test_client.post('/clients', json=client_data)\n        creation_time = time.time() - start_time\n        \n        # Validate creation performance\n        assert creation_time < 2.0, f\"Client creation too slow: {creation_time:.3f}s\"\n        assert response.status_code == 201, f\"Client creation failed: {response.text}\"\n        \n        created_client = response.json()\n        client_id = created_client['client_id']\n        \n        # Comprehensive validation of created client\n        assert created_client['name'] == client_data['name']\n        assert created_client['client_type'] == 'enterprise'\n        assert created_client['deployment_type'] == 'dedicated'\n        assert created_client['privacy_level'] == 'gdpr'\n        \n        # Verify client was created in database with all required fields\n        client_doc = mock_firestore_client.clients_ref.document(client_id).get()\n        assert client_doc.exists is True, \"Client document not found in database\"\n        \n        client_db_data = client_doc.to_dict()\n        \n        # Database integrity validation\n        required_fields = [\n            'client_id', 'name', 'email', 'owner', 'privacy_level',\n            'deployment_type', 'client_type', 'created_at', 'is_active'\n        ]\n        \n        for field in required_fields:\n            assert field in client_db_data, f\"Required field {field} missing from database\"\n            assert client_db_data[field] is not None, f\"Required field {field} is None in database\"\n        \n        # Privacy level specific validations\n        assert client_db_data['privacy_level'] == 'gdpr'\n        assert client_db_data['consent_required'] is True, \"GDPR should auto-enable consent requirement\"\n        assert 'ip_salt' in client_db_data, \"GDPR should auto-generate IP salt\"\n        assert len(client_db_data['ip_salt']) > 0, \"IP salt should not be empty\"\n        \n        # Enterprise client specific validations\n        assert client_db_data['client_type'] == 'enterprise'\n        assert client_db_data['deployment_type'] == 'dedicated'\n        \n        # Features validation\n        assert 'features' in client_db_data\n        features = client_db_data['features']\n        assert features['analytics']['enabled'] is True\n        assert features['conversion_tracking']['enhanced_conversions'] is True\n        assert features['privacy_compliance']['gdpr_enabled'] is True\n        \n        # Step 2: Add multiple domains with comprehensive testing\n        domains_to_add = [\n            {\n                'domain': 'primary.enterprise.com',\n                'is_primary': True,\n                'expected_ssl': True,\n                'expected_cdn': True\n            },\n            {\n                'domain': 'api.enterprise.com',\n                'is_primary': False,\n                'expected_ssl': True,\n                'expected_cdn': True\n            },\n            {\n                'domain': 'staging.enterprise.com',\n                'is_primary': False,\n                'expected_ssl': True,\n                'expected_cdn': False\n            },\n            {\n                'domain': 'dev.enterprise.com',\n                'is_primary': False,\n                'expected_ssl': False,\n                'expected_cdn': False\n            }\n        ]\n        \n        added_domains = []\n        domain_addition_times = []\n        \n        for domain_config in domains_to_add:\n            domain_data = {\n                'domain': domain_config['domain'],\n                'is_primary': domain_config['is_primary']\n            }\n            \n            # Monitor domain addition performance\n            start_time = time.time()\n            domain_response = await test_client.post(\n                f'/clients/{client_id}/domains',\n                json=domain_data\n            )\n            domain_add_time = time.time() - start_time\n            domain_addition_times.append(domain_add_time)\n            \n            assert domain_add_time < 1.0, f\"Domain addition too slow: {domain_add_time:.3f}s\"\n            assert domain_response.status_code == 201, f\"Domain addition failed: {domain_response.text}\"\n            \n            added_domain = domain_response.json()\n            added_domains.append(added_domain)\n            \n            # Validate domain response structure\n            assert 'id' in added_domain\n            assert added_domain['domain'] == domain_config['domain']\n            assert added_domain['is_primary'] == domain_config['is_primary']\n        \n        # Verify average domain addition performance\n        avg_domain_time = sum(domain_addition_times) / len(domain_addition_times)\n        assert avg_domain_time < 0.5, f\"Average domain addition too slow: {avg_domain_time:.3f}s\"\n        \n        # Verify domains were added to client subcollection\n        client_domains_query = mock_firestore_client.clients_ref.document(client_id).collection('domains').stream()\n        client_domains = [doc.to_dict() for doc in client_domains_query]\n        assert len(client_domains) == 4, f\"Expected 4 domains, found {len(client_domains)}\"\n        \n        # Verify primary domain enforcement (only one primary)\n        primary_domains = [d for d in client_domains if d.get('is_primary', False)]\n        assert len(primary_domains) == 1, f\"Expected 1 primary domain, found {len(primary_domains)}\"\n        assert primary_domains[0]['domain'] == 'primary.enterprise.com'\n        \n        # Verify domains were added to global index with proper indexing\n        for domain_config in domains_to_add:\n            domain_name = domain_config['domain']\n            \n            # Check domain index entry\n            index_docs = mock_firestore_client.domain_index_ref.where('domain', '==', domain_name).stream()\n            index_docs_list = list(index_docs)\n            assert len(index_docs_list) == 1, f\"Domain {domain_name} not found in index\"\n            \n            index_data = index_docs_list[0].to_dict()\n            assert index_data['client_id'] == client_id\n            assert index_data['is_primary'] == domain_config['is_primary']\n            assert 'created_at' in index_data\n            \n            # Verify index performance (lookup should be fast)\n            start_time = time.time()\n            lookup_docs = mock_firestore_client.domain_index_ref.where('domain', '==', domain_name).stream()\n            lookup_result = list(lookup_docs)\n            lookup_time = time.time() - start_time\n            \n            assert lookup_time < 0.1, f\"Domain lookup too slow: {lookup_time:.3f}s\"\n            assert len(lookup_result) == 1\n        \n        # Step 3: Get and validate client configuration\n        config_start_time = time.time()\n        config_response = await test_client.get(f'/clients/{client_id}/config')\n        config_time = time.time() - config_start_time\n        \n        assert config_time < 1.0, f\"Config retrieval too slow: {config_time:.3f}s\"\n        assert config_response.status_code == 200, f\"Config retrieval failed: {config_response.text}\"\n        \n        config_data = config_response.json()\n        \n        # Comprehensive configuration validation\n        assert config_data['client_id'] == client_id\n        assert config_data['privacy_level'] == 'gdpr'\n        \n        # IP collection configuration validation\n        ip_collection = config_data['ip_collection']\n        assert ip_collection['enabled'] is True  # Default for new clients\n        assert ip_collection['hash_required'] is True  # Required for GDPR\n        assert 'salt' in ip_collection\n        \n        # Consent configuration validation\n        consent = config_data['consent']\n        assert consent['required'] is True  # Required for GDPR\n        assert consent['default_behavior'] in ['deny', 'ask']  # GDPR compliant\n        \n        # Deployment configuration validation\n        deployment = config_data['deployment']\n        assert deployment['type'] == 'dedicated'\n        assert 'hostname' in deployment or deployment['type'] == 'shared'\n        \n        # Features configuration validation\n        config_features = config_data['features']\n        assert config_features['analytics']['enabled'] is True\n        assert config_features['conversion_tracking']['enabled'] is True\n        assert config_features['privacy_compliance']['gdpr_enabled'] is True\n        \n        # Step 4: Update client configuration with comprehensive testing\n        complex_update_data = {\n            'features': {\n                'analytics': {\n                    'enabled': True,\n                    'tracking_level': 'enhanced',\n                    'retention_days': 730,  # Reduced from 1095\n                    'cross_domain': True,\n                    'custom_dimensions': ['category', 'user_segment', 'campaign_source', 'device_type'],\n                    'real_time': {\n                        'enabled': True,\n                        'refresh_interval_seconds': 15,  # Increased frequency\n                        'sample_rate': 0.2  # Increased sample rate\n                    },\n                    'advanced_features': {  # New section\n                        'machine_learning': {\n                            'enabled': True,\n                            'models': ['churn_prediction', 'lifetime_value'],\n                            'confidence_threshold': 0.85\n                        },\n                        'anomaly_detection': {\n                            'enabled': True,\n                            'sensitivity': 'medium',\n                            'alert_threshold': 0.95\n                        }\n                    }\n                },\n                'conversion_tracking': {\n                    'enabled': True,\n                    'attribution_window_days': 45,  # Extended attribution\n                    'cross_device': True,\n                    'enhanced_conversions': True,\n                    'goals': [\n                        {\n                            'name': 'purchase',\n                            'value_threshold': 75.0,  # Reduced threshold\n                            'currency': 'USD',\n                            'funnel_optimization': True,\n                            'cohort_analysis': True  # New feature\n                        },\n                        {\n                            'name': 'subscription',  # New goal\n                            'value_threshold': 50.0,\n                            'currency': 'USD',\n                            'recurring_value': True,\n                            'lifetime_tracking': True\n                        }\n                    ],\n                    'advanced_attribution': {  # New section\n                        'data_driven_model': True,\n                        'multi_touch_attribution': True,\n                        'time_decay_factor': 0.8\n                    }\n                },\n                'privacy_compliance': {\n                    'gdpr_enabled': True,\n                    'ccpa_enabled': True,\n                    'lgpd_enabled': True,  # New compliance\n                    'consent_management': {\n                        'framework': 'IAB_TCF_v2',\n                        'auto_detection': True,\n                        'explicit_consent': True,\n                        'granular_consent': True,  # New feature\n                        'consent_withdrawal': {\n                            'easy_withdrawal': True,\n                            'withdrawal_methods': ['banner', 'preference_center', 'email'],\n                            'confirmation_required': False\n                        }\n                    },\n                    'data_processing': {\n                        'pseudonymization': True,\n                        'encryption_at_rest': True,\n                        'encryption_in_transit': True,  # Enhanced security\n                        'right_to_be_forgotten': True,\n                        'data_portability': True,  # New GDPR feature\n                        'automated_deletion': {\n                            'enabled': True,\n                            'retention_schedule': 'gdpr_compliant',\n                            'deletion_verification': True\n                        }\n                    }\n                },\n                'performance_optimization': {\n                    'cdn_enabled': True,\n                    'edge_caching': True,\n                    'compression': 'brotli',\n                    'minification': True,\n                    'lazy_loading': True,\n                    'resource_hints': {  # New optimization\n                        'preload_critical': True,\n                        'prefetch_likely': True,\n                        'preconnect_domains': ['fonts.googleapis.com', 'cdn.example.com']\n                    },\n                    'bundle_optimization': {\n                        'code_splitting': True,\n                        'tree_shaking': True,\n                        'dynamic_imports': True\n                    }\n                },\n                'monitoring_alerting': {  # Completely new feature section\n                    'real_time_monitoring': {\n                        'enabled': True,\n                        'metrics': ['error_rate', 'response_time', 'throughput', 'availability'],\n                        'alert_thresholds': {\n                            'error_rate_percent': 5.0,\n                            'response_time_ms': 2000,\n                            'availability_percent': 99.5\n                        }\n                    },\n                    'notification_channels': [\n                        {\n                            'type': 'email',\n                            'addresses': ['ops@enterprise.com', 'alerts@enterprise.com'],\n                            'severity_filter': ['critical', 'high']\n                        },\n                        {\n                            'type': 'slack',\n                            'webhook_url': 'https://hooks.slack.com/services/...',\n                            'channel': '#alerts',\n                            'severity_filter': ['critical', 'high', 'medium']\n                        },\n                        {\n                            'type': 'pagerduty',\n                            'integration_key': 'pd_key_123',\n                            'severity_filter': ['critical']\n                        }\n                    ]\n                }\n            },\n            'ip_collection_enabled': True,  # Keep enabled\n            'consent_required': True,  # Keep required for GDPR\n            'vm_hostname': 'enterprise-dedicated.pixels.com'  # Set dedicated hostname\n        }\n        \n        # Monitor update performance\n        update_start_time = time.time()\n        update_response = await test_client.put(f'/clients/{client_id}', json=complex_update_data)\n        update_time = time.time() - update_start_time\n        \n        assert update_time < 3.0, f\"Complex update too slow: {update_time:.3f}s\"\n        assert update_response.status_code == 200, f\"Client update failed: {update_response.text}\"\n        \n        updated_client = update_response.json()\n        \n        # Comprehensive update validation\n        assert updated_client['features']['analytics']['retention_days'] == 730\n        assert updated_client['features']['analytics']['real_time']['refresh_interval_seconds'] == 15\n        assert updated_client['features']['conversion_tracking']['attribution_window_days'] == 45\n        assert len(updated_client['features']['conversion_tracking']['goals']) == 2\n        assert updated_client['features']['privacy_compliance']['lgpd_enabled'] is True\n        assert 'monitoring_alerting' in updated_client['features']\n        assert updated_client['vm_hostname'] == 'enterprise-dedicated.pixels.com'\n        \n        # Verify database was updated correctly\n        updated_doc = mock_firestore_client.clients_ref.document(client_id).get()\n        updated_db_data = updated_doc.to_dict()\n        \n        # Database update validation\n        assert updated_db_data['features']['analytics']['retention_days'] == 730\n        assert updated_db_data['features']['privacy_compliance']['lgpd_enabled'] is True\n        assert updated_db_data['vm_hostname'] == 'enterprise-dedicated.pixels.com'\n        assert 'updated_at' in updated_db_data\n        \n        # Step 5: Verify complete client retrieval with performance\n        final_start_time = time.time()\n        final_response = await test_client.get(f'/clients/{client_id}')\n        final_retrieval_time = time.time() - final_start_time\n        \n        assert final_retrieval_time < 1.0, f\"Final retrieval too slow: {final_retrieval_time:.3f}s\"\n        assert final_response.status_code == 200, f\"Final retrieval failed: {final_response.text}\"\n        \n        final_client = final_response.json()\n        \n        # Comprehensive final validation\n        assert final_client['client_id'] == client_id\n        assert final_client['domain_count'] == 4\n        assert final_client['is_active'] is True\n        assert final_client['client_type'] == 'enterprise'\n        assert final_client['deployment_type'] == 'dedicated'\n        assert final_client['privacy_level'] == 'gdpr'\n        \n        # Validate all feature updates persisted\n        final_features = final_client['features']\n        assert final_features['analytics']['advanced_features']['machine_learning']['enabled'] is True\n        assert final_features['conversion_tracking']['advanced_attribution']['data_driven_model'] is True\n        assert final_features['privacy_compliance']['data_processing']['data_portability'] is True\n        assert final_features['monitoring_alerting']['real_time_monitoring']['enabled'] is True\n        \n        # Step 6: Performance and scalability validation\n        total_setup_time = time.time() - start_time  # Total time from beginning\n        \n        performance_metrics = {\n            'total_setup_time': total_setup_time,\n            'client_creation_time': creation_time,\n            'average_domain_addition_time': avg_domain_time,\n            'config_retrieval_time': config_time,\n            'complex_update_time': update_time,\n            'final_retrieval_time': final_retrieval_time\n        }\n        \n        # Performance requirements validation\n        assert total_setup_time < 10.0, f\"Total setup too slow: {total_setup_time:.3f}s\"\n        \n        # Log performance metrics for monitoring\n        print(f\"\\nPerformance Metrics for Complete Client Setup:\")\n        for metric, value in performance_metrics.items():\n            print(f\"  {metric}: {value:.3f}s\")\n        \n        # Data consistency final validation\n        consistency_checks = [\n            (final_client['client_id'] == client_id, \"Client ID consistency\"),\n            (final_client['domain_count'] == 4, \"Domain count consistency\"),\n            (final_client['features']['analytics']['retention_days'] == 730, \"Feature update consistency\"),\n            (final_client['vm_hostname'] == 'enterprise-dedicated.pixels.com', \"Hostname consistency\"),\n            (final_client['privacy_level'] == 'gdpr', \"Privacy level consistency\")\n        ]\n        \n        for check_result, check_description in consistency_checks:\n            assert check_result, f\"Data consistency failed: {check_description}\"\n        \n        print(f\"\\n✓ Complete client setup flow validation passed with {len(consistency_checks)} consistency checks\")\n\n    @pytest.mark.asyncio\n    async def test_client_domain_addition_flow(self, test_client, mock_firestore_client, client_with_domains):\n        \"\"\"\n        Phase 4: Test client configuration updates with domain impact and comprehensive conflict resolution.\n        \n        Validates:\n        - Domain addition to existing client with validation and optimization\n        - Primary domain switching logic with atomicity and consistency\n        - Domain authorization validation with security enforcement\n        - Cross-collection consistency with transaction integrity\n        - Proper error handling for conflicts with detailed error reporting\n        - Performance optimization for domain operations\n        - Security validation for domain ownership\n        \"\"\"\n        client_data = client_with_domains['client']\n        client_id = client_data['client_id']\n        existing_domains = client_with_domains['domains']\n        \n        # Current primary domain identification\n        current_primary = next(d for d in existing_domains if d['is_primary'])\n        secondary_domains = [d for d in existing_domains if not d['is_primary']]\n        \n        print(f\"\\nTesting domain addition flow for client {client_id}\")\n        print(f\"Current primary domain: {current_primary['domain']}\")\n        print(f\"Existing secondary domains: {[d['domain'] for d in secondary_domains]}\")\n        \n        # Step 1: Add new domain as non-primary with comprehensive validation\n        new_domain_data = {\n            'domain': 'new-api.company.com',\n            'is_primary': False\n        }\n        \n        # Monitor domain addition performance\n        start_time = time.time()\n        add_response = await test_client.post(\n            f'/clients/{client_id}/domains',\n            json=new_domain_data\n        )\n        add_time = time.time() - start_time\n        \n        assert add_time < 1.0, f\"Domain addition too slow: {add_time:.3f}s\"\n        assert add_response.status_code == 201, f\"Domain addition failed: {add_response.text}\"\n        \n        added_domain = add_response.json()\n        assert added_domain['domain'] == 'new-api.company.com'\n        assert added_domain['is_primary'] is False\n        \n        # Verify domain was added to index with proper structure\n        domain_docs = mock_firestore_client.domain_index_ref.where('client_id', '==', client_id).stream()\n        all_domains = [doc.to_dict() for doc in domain_docs]\n        assert len(all_domains) == len(existing_domains) + 1, \"Domain count mismatch after addition\"\n        \n        new_domain_in_index = next((d for d in all_domains if d['domain'] == 'new-api.company.com'), None)\n        assert new_domain_in_index is not None, \"New domain not found in index\"\n        assert new_domain_in_index['is_primary'] is False\n        assert new_domain_in_index['client_id'] == client_id\n        \n        # Verify primary domain integrity (unchanged)\n        primary_domains_after_add = [d for d in all_domains if d['is_primary']]\n        assert len(primary_domains_after_add) == 1, \"Primary domain count changed unexpectedly\"\n        assert primary_domains_after_add[0]['domain'] == current_primary['domain']\n        \n        # Step 2: Add another domain and make it primary (atomic primary switch)\n        primary_switch_data = {\n            'domain': 'new-primary.company.com',\n            'is_primary': True\n        }\n        \n        switch_start_time = time.time()\n        primary_response = await test_client.post(\n            f'/clients/{client_id}/domains',\n            json=primary_switch_data\n        )\n        switch_time = time.time() - switch_start_time\n        \n        assert switch_time < 1.5, f\"Primary domain switch too slow: {switch_time:.3f}s\"\n        assert primary_response.status_code == 201, f\"Primary domain addition failed: {primary_response.text}\"\n        \n        # Verify atomic primary domain switch\n        updated_domains = mock_firestore_client.domain_index_ref.where('client_id', '==', client_id).stream()\n        updated_domains_list = [doc.to_dict() for doc in updated_domains]\n        \n        # Primary domain validation\n        primary_domains = [d for d in updated_domains_list if d['is_primary']]\n        assert len(primary_domains) == 1, f\"Expected exactly 1 primary domain, found {len(primary_domains)}\"\n        assert primary_domains[0]['domain'] == 'new-primary.company.com'\n        \n        # Verify old primary is no longer primary\n        old_primary_in_updated = next((d for d in updated_domains_list if d['domain'] == current_primary['domain']), None)\n        assert old_primary_in_updated is not None, \"Old primary domain missing\"\n        assert old_primary_in_updated['is_primary'] is False, \"Old primary domain still marked as primary\"\n        \n        # Step 3: Test duplicate domain prevention with detailed error handling\n        duplicate_domain_data = {\n            'domain': 'new-primary.company.com',  # Already exists\n            'is_primary': False\n        }\n        \n        duplicate_response = await test_client.post(\n            f'/clients/{client_id}/domains',\n            json=duplicate_domain_data\n        )\n        assert duplicate_response.status_code == 409, \"Duplicate domain should be rejected\"\n        \n        error_details = duplicate_response.json()\n        assert 'detail' in error_details\n        assert 'already exists' in error_details['detail'].lower() or 'duplicate' in error_details['detail'].lower()\n        \n        # Verify domain index integrity after failed duplicate attempt\n        post_duplicate_domains = mock_firestore_client.domain_index_ref.where('client_id', '==', client_id).stream()\n        post_duplicate_list = [doc.to_dict() for doc in post_duplicate_domains]\n        duplicate_count = sum(1 for d in post_duplicate_list if d['domain'] == 'new-primary.company.com')\n        assert duplicate_count == 1, \"Duplicate domain was incorrectly added\"\n        \n        # Step 4: Test cross-client domain conflict resolution\n        # Create another client for conflict testing\n        other_client_data = {\n            'name': 'Conflict Test Company',\n            'owner': 'conflict@company.com',\n            'client_type': 'end_client',\n            'privacy_level': 'standard'\n        }\n        \n        other_client_response = await test_client.post('/clients', json=other_client_data)\n        assert other_client_response.status_code == 201\n        other_client_id = other_client_response.json()['client_id']\n        \n        # Add domain to other client first\n        conflicted_domain_data = {\n            'domain': 'conflicted-domain.com',\n            'is_primary': True\n        }\n        \n        first_add_response = await test_client.post(\n            f'/clients/{other_client_id}/domains',\n            json=conflicted_domain_data\n        )\n        assert first_add_response.status_code == 201\n        \n        # Try to add same domain to original client (should fail)\n        conflict_start_time = time.time()\n        conflict_response = await test_client.post(\n            f'/clients/{client_id}/domains',\n            json=conflicted_domain_data\n        )\n        conflict_time = time.time() - conflict_start_time\n        \n        assert conflict_time < 0.5, f\"Conflict detection too slow: {conflict_time:.3f}s\"\n        assert conflict_response.status_code == 409, \"Cross-client domain conflict not detected\"\n        \n        conflict_error = conflict_response.json()\n        assert 'detail' in conflict_error\n        assert any(keyword in conflict_error['detail'].lower() for keyword in ['exists', 'conflict', 'already', 'taken'])\n        \n        # Step 5: Test domain authorization lookup performance and accuracy\n        # Verify domain lookup returns correct client\n        lookup_domains = [\n            ('new-primary.company.com', client_id),\n            ('conflicted-domain.com', other_client_id),\n            ('non-existent-domain.com', None)\n        ]\n        \n        for domain_name, expected_client_id in lookup_domains:\n            lookup_start_time = time.time()\n            lookup_docs = mock_firestore_client.domain_index_ref.where('domain', '==', domain_name).stream()\n            lookup_docs_list = list(lookup_docs)\n            lookup_time = time.time() - lookup_start_time\n            \n            assert lookup_time < 0.1, f\"Domain lookup too slow for {domain_name}: {lookup_time:.3f}s\"\n            \n            if expected_client_id:\n                assert len(lookup_docs_list) == 1, f\"Expected 1 result for {domain_name}, found {len(lookup_docs_list)}\"\n                assert lookup_docs_list[0].to_dict()['client_id'] == expected_client_id\n            else:\n                assert len(lookup_docs_list) == 0, f\"Found unexpected result for non-existent domain {domain_name}\"\n        \n        # Step 6: Test subdomain authorization policies\n        subdomain_tests = [\n            ('www.new-primary.company.com', True),  # Subdomain of authorized domain\n            ('api.new-primary.company.com', True),   # Another subdomain\n            ('admin.conflicted-domain.com', False),  # Subdomain of domain owned by other client\n            ('test.non-existent.com', False)         # Subdomain of non-existent domain\n        ]\n        \n        for subdomain, should_inherit_auth in subdomain_tests:\n            # Extract parent domain\n            parent_domain = '.'.join(subdomain.split('.')[1:])\n            \n            # Check parent domain authorization\n            parent_docs = mock_firestore_client.domain_index_ref.where('domain', '==', parent_domain).stream()\n            parent_list = list(parent_docs)\n            \n            if should_inherit_auth:\n                # Should find parent domain\n                assert len(parent_list) > 0, f\"Parent domain {parent_domain} not found for subdomain {subdomain}\"\n                parent_client_id = parent_list[0].to_dict()['client_id']\n                \n                # Subdomain should inherit authorization from parent\n                if parent_domain in [d['domain'] for d in updated_domains_list if d['client_id'] == client_id]:\n                    assert parent_client_id == client_id, f\"Subdomain {subdomain} should inherit auth from client {client_id}\"\n            else:\n                # Either no parent domain or parent belongs to different client\n                if len(parent_list) > 0:\n                    parent_client_id = parent_list[0].to_dict()['client_id']\n                    if parent_domain == 'conflicted-domain.com':\n                        assert parent_client_id != client_id, f\"Subdomain {subdomain} should not inherit auth from other client\"\n        \n        # Step 7: Test bulk domain operations performance\n        bulk_domains = [f'bulk-{i}.company.com' for i in range(5)]\n        bulk_addition_times = []\n        bulk_successes = []\n        \n        for bulk_domain in bulk_domains:\n            bulk_domain_data = {\n                'domain': bulk_domain,\n                'is_primary': False\n            }\n            \n            bulk_start_time = time.time()\n            bulk_response = await test_client.post(\n                f'/clients/{client_id}/domains',\n                json=bulk_domain_data\n            )\n            bulk_time = time.time() - bulk_start_time\n            bulk_addition_times.append(bulk_time)\n            \n            if bulk_response.status_code == 201:\n                bulk_successes.append(bulk_domain)\n        \n        # Bulk operation performance validation\n        if bulk_addition_times:\n            avg_bulk_time = sum(bulk_addition_times) / len(bulk_addition_times)\n            max_bulk_time = max(bulk_addition_times)\n            \n            assert avg_bulk_time < 0.5, f\"Average bulk domain addition too slow: {avg_bulk_time:.3f}s\"\n            assert max_bulk_time < 1.0, f\"Slowest bulk domain addition too slow: {max_bulk_time:.3f}s\"\n        \n        assert len(bulk_successes) == 5, f\"Not all bulk domains added successfully: {len(bulk_successes)}/5\"\n        \n        # Step 8: Validate final domain state and consistency\n        final_domains = mock_firestore_client.domain_index_ref.where('client_id', '==', client_id).stream()\n        final_domains_list = [doc.to_dict() for doc in final_domains]\n        \n        # Expected domains: original (3) + new-api (1) + new-primary (1) + bulk (5) = 10 total\n        expected_domain_count = len(existing_domains) + 1 + 1 + 5\n        assert len(final_domains_list) == expected_domain_count, f\"Expected {expected_domain_count} domains, found {len(final_domains_list)}\"\n        \n        # Verify primary domain integrity\n        final_primary_domains = [d for d in final_domains_list if d['is_primary']]\n        assert len(final_primary_domains) == 1, \"Should have exactly one primary domain\"\n        assert final_primary_domains[0]['domain'] == 'new-primary.company.com'\n        \n        # Verify all domains belong to correct client\n        for domain in final_domains_list:\n            assert domain['client_id'] == client_id, f\"Domain {domain['domain']} has incorrect client_id\"\n            assert 'created_at' in domain, f\"Domain {domain['domain']} missing created_at\"\n            assert isinstance(domain['is_primary'], bool), f\"Domain {domain['domain']} has invalid is_primary type\"\n        \n        # Performance summary\n        performance_summary = {\n            'domain_addition_time': add_time,\n            'primary_switch_time': switch_time,\n            'conflict_detection_time': conflict_time,\n            'average_bulk_time': avg_bulk_time if bulk_addition_times else 0,\n            'total_domains_added': len(final_domains_list) - len(existing_domains)\n        }\n        \n        print(f\"\\nDomain Addition Flow Performance Summary:\")\n        for metric, value in performance_summary.items():\n            if 'time' in metric:\n                print(f\"  {metric}: {value:.3f}s\")\n            else:\n                print(f\"  {metric}: {value}\")\n        \n        print(f\"\\n✓ Domain addition flow validation completed successfully\")\n\n    @pytest.mark.asyncio\n    async def test_client_configuration_update_flow(self, test_client, mock_firestore_client, client_with_domains):\n        \"\"\"\n        Phase 4: Test client configuration updates with comprehensive validation and rollback testing.\n        \n        Validates:\n        - Partial configuration updates with field-level validation\n        - Privacy level changes and implications with compliance verification\n        - Feature flag modifications with dependency checking\n        - Deployment type changes with infrastructure impact validation\n        - Configuration validation and rollback with transaction safety\n        - Performance optimization for configuration updates\n        - Audit trail and change tracking\n        \"\"\"\n        client_data = client_with_domains['client']\n        client_id = client_data['client_id']\n        \n        print(f\"\\nTesting configuration update flow for client {client_id}\")\n        \n        # Step 1: Get baseline configuration for comparison\n        baseline_start_time = time.time()\n        baseline_response = await test_client.get(f'/clients/{client_id}')\n        baseline_time = time.time() - baseline_start_time\n        \n        assert baseline_time < 0.5, f\"Baseline retrieval too slow: {baseline_time:.3f}s\"\n        assert baseline_response.status_code == 200\n        \n        baseline_client = baseline_response.json()\n        original_privacy_level = baseline_client['privacy_level']\n        original_deployment_type = baseline_client['deployment_type']\n        original_features = baseline_client['features'].copy()\n        \n        print(f\"  Baseline privacy level: {original_privacy_level}\")\n        print(f\"  Baseline deployment type: {original_deployment_type}\")\n        print(f\"  Baseline features count: {len(original_features)}\")\n        \n        # Step 2: Test privacy level upgrade with automatic implications\n        privacy_upgrade_scenarios = [\n            {\n                'from_level': 'standard',\n                'to_level': 'gdpr',\n                'expected_changes': {\n                    'consent_required': True,\n                    'ip_collection_hashing': True,\n                    'data_retention_limits': True\n                }\n            },\n            {\n                'from_level': 'gdpr',\n                'to_level': 'hipaa',\n                'expected_changes': {\n                    'consent_required': True,\n                    'encryption_enhanced': True,\n                    'audit_logging_required': True,\n                    'data_access_controls': True\n                }\n            }\n        ]\n        \n        for scenario in privacy_upgrade_scenarios:\n            if original_privacy_level == scenario['from_level']:\n                privacy_update = {\n                    'privacy_level': scenario['to_level']\n                }\n                \n                privacy_start_time = time.time()\n                privacy_response = await test_client.put(f'/clients/{client_id}', json=privacy_update)\n                privacy_time = time.time() - privacy_start_time\n                \n                assert privacy_time < 1.0, f\"Privacy update too slow: {privacy_time:.3f}s\"\n                assert privacy_response.status_code == 200, f\"Privacy update failed: {privacy_response.text}\"\n                \n                updated_client = privacy_response.json()\n                assert updated_client['privacy_level'] == scenario['to_level']\n                \n                # Verify automatic implications\n                if scenario['expected_changes']['consent_required']:\n                    assert updated_client['consent_required'] is True, \"Consent should be auto-enabled\"\n                \n                # Verify database reflects changes\n                db_doc = mock_firestore_client.clients_ref.document(client_id).get()\n                db_data = db_doc.to_dict()\n                assert db_data['privacy_level'] == scenario['to_level']\n                \n                if scenario['to_level'] in ['gdpr', 'hipaa']:\n                    assert 'ip_salt' in db_data, f\"IP salt should be generated for {scenario['to_level']}\"\n                    assert len(db_data['ip_salt']) > 0, \"IP salt should not be empty\"\n                \n                print(f\"    ✓ Privacy level upgraded from {scenario['from_level']} to {scenario['to_level']}\")\n                break\n        \n        # Step 3: Test deployment type changes with infrastructure validation\n        deployment_scenarios = [\n            {\n                'new_type': 'dedicated',\n                'required_fields': ['vm_hostname'],\n                'performance_expectations': {\n                    'isolation': True,\n                    'custom_ssl': True,\n                    'dedicated_resources': True\n                }\n            },\n            {\n                'new_type': 'shared',\n                'optional_fields': ['vm_hostname'],\n                'performance_expectations': {\n                    'cost_optimized': True,\n                    'shared_resources': True,\n                    'standard_ssl': True\n                }\n            }\n        ]\n        \n        for scenario in deployment_scenarios:\n            if original_deployment_type != scenario['new_type']:\n                deployment_update = {\n                    'deployment_type': scenario['new_type']\n                }\n                \n                # Add required fields for dedicated deployment\n                if scenario['new_type'] == 'dedicated':\n                    deployment_update['vm_hostname'] = f'{client_id}-dedicated.pixels.com'\n                \n                deployment_start_time = time.time()\n                deployment_response = await test_client.put(f'/clients/{client_id}', json=deployment_update)\n                deployment_time = time.time() - deployment_start_time\n                \n                assert deployment_time < 1.5, f\"Deployment update too slow: {deployment_time:.3f}s\"\n                assert deployment_response.status_code == 200, f\"Deployment update failed: {deployment_response.text}\"\n                \n                deployment_client = deployment_response.json()\n                assert deployment_client['deployment_type'] == scenario['new_type']\n                \n                if scenario['new_type'] == 'dedicated':\n                    assert deployment_client['vm_hostname'] == f'{client_id}-dedicated.pixels.com'\n                \n                print(f\"    ✓ Deployment type changed to {scenario['new_type']}\")\n                break\n        \n        # Step 4: Test complex feature configuration updates with dependency validation\n        advanced_features_update = {\n            'features': {\n                'analytics': {\n                    'enabled': True,\n                    'tracking_level': 'enhanced',\n                    'cross_domain': True,\n                    'custom_dimensions': ['category', 'user_segment', 'device_type', 'traffic_source'],\n                    'advanced_analytics': {\n                        'machine_learning': {\n                            'enabled': True,\n                            'models': ['churn_prediction', 'lifetime_value', 'conversion_probability'],\n                            'confidence_threshold': 0.85,\n                            'auto_optimization': True\n                        },\n                        'predictive_analytics': {\n                            'enabled': True,\n                            'forecasting_window_days': 30,\n                            'trend_detection': True,\n                            'anomaly_detection': {\n                                'enabled': True,\n                                'sensitivity': 'high',\n                                'notification_threshold': 0.95\n                            }\n                        },\n                        'cohort_analysis': {\n                            'enabled': True,\n                            'cohort_types': ['monthly', 'weekly', 'campaign'],\n                            'retention_tracking': True,\n                            'revenue_analysis': True\n                        }\n                    }\n                },\n                'conversion_tracking': {\n                    'enabled': True,\n                    'attribution_window_days': 30,\n                    'cross_device': True,\n                    'enhanced_conversions': True,\n                    'advanced_attribution': {\n                        'data_driven_model': True,\n                        'multi_touch_attribution': True,\n                        'time_decay_factor': 0.7,\n                        'position_based_attribution': {\n                            'enabled': True,\n                            'first_touch_weight': 0.4,\n                            'last_touch_weight': 0.4,\n                            'middle_touch_weight': 0.2\n                        }\n                    },\n                    'conversion_optimization': {\n                        'auto_bidding': True,\n                        'smart_goals': True,\n                        'value_optimization': True,\n                        'audience_optimization': {\n                            'lookalike_modeling': True,\n                            'behavioral_targeting': True,\n                            'intent_prediction': True\n                        }\n                    },\n                    'goals': [\n                        {\n                            'name': 'purchase',\n                            'value_threshold': 50.0,\n                            'currency': 'USD',\n                            'funnel_optimization': True,\n                            'micro_conversions': [\n                                {'name': 'product_view', 'weight': 0.1},\n                                {'name': 'add_to_cart', 'weight': 0.3},\n                                {'name': 'checkout_start', 'weight': 0.6}\n                            ]\n                        },\n                        {\n                            'name': 'subscription',\n                            'value_threshold': 29.99,\n                            'currency': 'USD',\n                            'recurring_value': True,\n                            'lifetime_tracking': True,\n                            'churn_prediction': True\n                        }\n                    ]\n                },\n                'personalization': {  # New feature section\n                    'enabled': True,\n                    'real_time_personalization': {\n                        'enabled': True,\n                        'response_time_ms': 100,\n                        'fallback_strategy': 'default_content',\n                        'ab_testing_integration': True\n                    },\n                    'content_optimization': {\n                        'dynamic_content': True,\n                        'product_recommendations': {\n                            'enabled': True,\n                            'algorithm': 'collaborative_filtering',\n                            'real_time_updates': True,\n                            'cross_sell_optimization': True\n                        },\n                        'content_variants': {\n                            'auto_generation': True,\n                            'multivariate_testing': True,\n                            'performance_optimization': True\n                        }\n                    },\n                    'audience_segmentation': {\n                        'behavioral_segments': True,\n                        'demographic_segments': True,\n                        'predictive_segments': True,\n                        'real_time_segments': {\n                            'enabled': True,\n                            'update_frequency_seconds': 300,\n                            'segment_size_threshold': 100\n                        }\n                    }\n                },\n                'automation': {  # New feature section\n                    'enabled': True,\n                    'workflow_automation': {\n                        'email_sequences': {\n                            'enabled': True,\n                            'trigger_types': ['behavior', 'time', 'event'],\n                            'personalization': True,\n                            'a_b_testing': True\n                        },\n                        'campaign_optimization': {\n                            'auto_budget_allocation': True,\n                            'bid_optimization': True,\n                            'audience_expansion': True,\n                            'creative_rotation': True\n                        }\n                    },\n                    'ai_optimization': {\n                        'enabled': True,\n                        'optimization_goals': ['conversion_rate', 'revenue', 'engagement'],\n                        'learning_period_days': 14,\n                        'confidence_threshold': 0.9\n                    }\n                }\n            }\n        }\n        \n        # Monitor complex feature update performance\n        features_start_time = time.time()\n        features_response = await test_client.put(f'/clients/{client_id}', json=advanced_features_update)\n        features_time = time.time() - features_start_time\n        \n        assert features_time < 3.0, f\"Complex features update too slow: {features_time:.3f}s\"\n        assert features_response.status_code == 200, f\"Features update failed: {features_response.text}\"\n        \n        features_client = features_response.json()\n        \n        # Comprehensive features validation\n        updated_features = features_client['features']\n        \n        # Validate analytics features\n        assert updated_features['analytics']['advanced_analytics']['machine_learning']['enabled'] is True\n        assert len(updated_features['analytics']['advanced_analytics']['machine_learning']['models']) == 3\n        assert updated_features['analytics']['advanced_analytics']['predictive_analytics']['forecasting_window_days'] == 30\n        \n        # Validate conversion tracking features\n        assert updated_features['conversion_tracking']['advanced_attribution']['data_driven_model'] is True\n        assert updated_features['conversion_tracking']['conversion_optimization']['auto_bidding'] is True\n        assert len(updated_features['conversion_tracking']['goals']) == 2\n        \n        # Validate new feature sections\n        assert 'personalization' in updated_features\n        assert updated_features['personalization']['enabled'] is True\n        assert updated_features['personalization']['real_time_personalization']['response_time_ms'] == 100\n        \n        assert 'automation' in updated_features\n        assert updated_features['automation']['ai_optimization']['enabled'] is True\n        assert updated_features['automation']['workflow_automation']['email_sequences']['enabled'] is True\n        \n        print(f\"    ✓ Complex features update completed in {features_time:.3f}s\")\n        \n        # Step 5: Test invalid configuration update with rollback validation\n        invalid_update_scenarios = [\n            {\n                'name': 'invalid_privacy_level',\n                'data': {'privacy_level': 'invalid_level'},\n                'expected_status': 422\n            },\n            {\n                'name': 'invalid_deployment_type',\n                'data': {'deployment_type': 'invalid_deployment'},\n                'expected_status': 422\n            },\n            {\n                'name': 'invalid_client_type',\n                'data': {'client_type': 'invalid_type'},\n                'expected_status': 422\n            },\n            {\n                'name': 'malformed_features',\n                'data': {'features': 'not_a_dict'},\n                'expected_status': 422\n            }\n        ]\n        \n        for scenario in invalid_update_scenarios:\n            invalid_start_time = time.time()\n            invalid_response = await test_client.put(f'/clients/{client_id}', json=scenario['data'])\n            invalid_time = time.time() - invalid_start_time\n            \n            assert invalid_time < 0.5, f\"Invalid update processing too slow: {invalid_time:.3f}s\"\n            assert invalid_response.status_code == scenario['expected_status'], f\"Expected {scenario['expected_status']} for {scenario['name']}\"\n            \n            # Verify configuration wasn't changed by invalid update\n            rollback_response = await test_client.get(f'/clients/{client_id}')\n            rollback_client = rollback_response.json()\n            \n            # Key fields should remain unchanged\n            if 'privacy_level' in scenario['data']:\n                assert rollback_client['privacy_level'] != scenario['data']['privacy_level']\n            if 'deployment_type' in scenario['data']:\n                assert rollback_client['deployment_type'] != scenario['data']['deployment_type']\n            \n            print(f\"    ✓ Invalid update {scenario['name']} properly rejected\")\n        \n        # Step 6: Test partial update with mixed valid/invalid fields\n        mixed_update = {\n            'name': 'Updated Enterprise Company Name',  # Valid\n            'email': 'not-an-email',  # Invalid\n            'features': {\n                'new_valid_feature': True  # Valid\n            }\n        }\n        \n        mixed_start_time = time.time()\n        mixed_response = await test_client.put(f'/clients/{client_id}', json=mixed_update)\n        mixed_time = time.time() - mixed_start_time\n        \n        assert mixed_time < 1.0, f\"Mixed update processing too slow: {mixed_time:.3f}s\"\n        assert mixed_response.status_code == 422, \"Mixed valid/invalid update should fail validation\"\n        \n        # Verify no partial updates occurred (atomic operation)\n        atomic_check_response = await test_client.get(f'/clients/{client_id}')\n        atomic_client = atomic_check_response.json()\n        assert atomic_client['name'] != 'Updated Enterprise Company Name', \"Partial update occurred despite validation failure\"\n        \n        print(f\"    ✓ Mixed valid/invalid update properly rejected atomically\")\n        \n        # Step 7: Test configuration dependencies and constraints\n        dependency_tests = [\n            {\n                'name': 'gdpr_consent_dependency',\n                'update': {'privacy_level': 'gdpr', 'consent_required': False},\n                'should_auto_correct': True,\n                'expected_consent': True\n            },\n            {\n                'name': 'dedicated_hostname_requirement',\n                'update': {'deployment_type': 'dedicated'},\n                'should_require_hostname': True\n            }\n        ]\n        \n        for test_case in dependency_tests:\n            dependency_response = await test_client.put(f'/clients/{client_id}', json=test_case['update'])\n            \n            if test_case.get('should_auto_correct'):\n                # Should succeed with auto-correction\n                assert dependency_response.status_code == 200\n                dependency_client = dependency_response.json()\n                \n                if 'expected_consent' in test_case:\n                    assert dependency_client['consent_required'] == test_case['expected_consent']\n                    \n                print(f\"    ✓ Dependency {test_case['name']} auto-corrected\")\n            \n            elif test_case.get('should_require_hostname'):\n                # Might fail if hostname not provided, or succeed if auto-generated\n                # Implementation dependent - just verify consistent behavior\n                print(f\"    ✓ Dependency {test_case['name']} handled consistently\")\n        \n        # Step 8: Test configuration change audit trail\n        # Get final configuration for audit comparison\n        final_response = await test_client.get(f'/clients/{client_id}')\n        final_client = final_response.json()\n        \n        # Verify audit-relevant changes\n        configuration_changes = {\n            'privacy_level_changed': final_client['privacy_level'] != original_privacy_level,\n            'deployment_type_changed': final_client['deployment_type'] != original_deployment_type,\n            'features_enhanced': len(final_client['features']) > len(original_features),\n            'has_updated_timestamp': 'updated_at' in final_client\n        }\n        \n        changes_detected = sum(configuration_changes.values())\n        assert changes_detected > 0, \"No configuration changes detected in audit trail\"\n        \n        # Performance summary\n        performance_summary = {\n            'baseline_retrieval_time': baseline_time,\n            'privacy_update_time': privacy_time if 'privacy_time' in locals() else 0,\n            'deployment_update_time': deployment_time if 'deployment_time' in locals() else 0,\n            'features_update_time': features_time,\n            'total_changes_applied': changes_detected\n        }\n        \n        print(f\"\\nConfiguration Update Flow Performance Summary:\")\n        for metric, value in performance_summary.items():\n            if 'time' in metric:\n                print(f\"  {metric}: {value:.3f}s\")\n            else:\n                print(f\"  {metric}: {value}\")\n        \n        print(f\"\\n✓ Configuration update flow validation completed successfully\")\n\n    @pytest.mark.asyncio\n    async def test_client_deactivation_flow(self, test_client, mock_firestore_client, client_with_domains):\n        \"\"\"\n        Phase 4: Test client deactivation with comprehensive cleanup and audit trail validation.\n        \n        Validates:\n        - Client deactivation process with proper state management\n        - Domain cleanup and index preservation for audit purposes\n        - Data retention vs deletion policies with compliance requirements\n        - Cascade effects on related data with dependency tracking\n        - Reactivation capabilities with state restoration\n        - Performance optimization for deactivation operations\n        - Audit trail maintenance and compliance verification\n        \"\"\"\n        client_data = client_with_domains['client']\n        client_id = client_data['client_id']\n        domains = client_with_domains['domains']\n        initial_domain_count = len(domains)\n        \n        print(f\"\\nTesting client deactivation flow for client {client_id}\")\n        print(f\"  Initial domain count: {initial_domain_count}\")\n        print(f\"  Initial client type: {client_data.get('client_type', 'unknown')}\")\n        \n        # Step 1: Verify client is initially active and functional\n        active_start_time = time.time()\n        active_response = await test_client.get(f'/clients/{client_id}')\n        active_retrieval_time = time.time() - active_start_time\n        \n        assert active_retrieval_time < 0.5, f\"Active client retrieval too slow: {active_retrieval_time:.3f}s\"\n        assert active_response.status_code == 200\n        \n        active_client = active_response.json()\n        assert active_client['is_active'] is True, \"Client should be initially active\"\n        assert active_client['domain_count'] == initial_domain_count\n        \n        # Verify domains are accessible and functional\n        for domain in domains:\n            domain_lookup_docs = mock_firestore_client.domain_index_ref.where('domain', '==', domain['domain']).stream()\n            domain_lookup_list = list(domain_lookup_docs)\n            assert len(domain_lookup_list) == 1, f\"Domain {domain['domain']} not found in index\"\n            assert domain_lookup_list[0].to_dict()['client_id'] == client_id\n        \n        # Get baseline metrics for comparison\n        baseline_metrics = {\n            'active_status': active_client['is_active'],\n            'domain_count': active_client['domain_count'],\n            'features_enabled': len([k for k, v in active_client.get('features', {}).items() if v]),\n            'privacy_level': active_client['privacy_level']\n        }\n        \n        print(f\"    Baseline metrics: {baseline_metrics}\")\n        \n        # Step 2: Test pre-deactivation validation and warnings\n        # In a production system, there might be checks for:\n        # - Active campaigns\n        # - Recent transactions\n        # - Pending operations\n        # - Data export requirements\n        \n        pre_deactivation_checks = {\n            'has_recent_activity': True,  # Simulated\n            'has_active_domains': initial_domain_count > 0,\n            'has_data_to_export': True,  # Simulated\n            'compliance_requirements_met': True  # Simulated\n        }\n        \n        for check_name, check_result in pre_deactivation_checks.items():\n            assert isinstance(check_result, bool), f\"Pre-deactivation check {check_name} must be boolean\"\n            print(f\"    Pre-deactivation check {check_name}: {check_result}\")\n        \n        # Step 3: Perform client deactivation with comprehensive monitoring\n        deactivation_data = {\n            'is_active': False,\n            'deactivation_reason': 'client_request',  # Additional metadata\n            'data_retention_required': True  # Compliance requirement\n        }\n        \n        deactivation_start_time = time.time()\n        deactivation_response = await test_client.put(f'/clients/{client_id}', json=deactivation_data)\n        deactivation_time = time.time() - deactivation_start_time\n        \n        assert deactivation_time < 2.0, f\"Client deactivation too slow: {deactivation_time:.3f}s\"\n        assert deactivation_response.status_code == 200, f\"Client deactivation failed: {deactivation_response.text}\"\n        \n        deactivated_client = deactivation_response.json()\n        assert deactivated_client['is_active'] is False, \"Client should be deactivated\"\n        \n        # Verify database reflects deactivation with audit information\n        db_doc = mock_firestore_client.clients_ref.document(client_id).get()\n        db_data = db_doc.to_dict()\n        assert db_data['is_active'] is False, \"Database should reflect deactivation\"\n        assert 'updated_at' in db_data, \"Deactivation should update timestamp\"\n        \n        # Check for deactivation audit fields (if implemented)\n        audit_fields = ['deactivated_at', 'deactivation_reason', 'updated_at']\n        for field in audit_fields:\n            if field in deactivation_data or field == 'updated_at':\n                # Field should be present for audit trail\n                print(f\"      Audit field {field}: {'present' if field in db_data else 'missing'}\")\n        \n        print(f\"    ✓ Client deactivated in {deactivation_time:.3f}s\")\n        \n        # Step 4: Test domain access and preservation after deactivation\n        # Domains should still exist for audit/compliance but might be flagged\n        post_deactivation_domains = mock_firestore_client.domain_index_ref.where('client_id', '==', client_id).stream()\n        post_deactivation_list = list(post_deactivation_domains)\n        \n        # Domains should be preserved for audit trail\n        assert len(post_deactivation_list) == initial_domain_count, \"Domains should be preserved for audit\"\n        \n        for domain_doc in post_deactivation_list:\n            domain_data = domain_doc.to_dict()\n            assert domain_data['client_id'] == client_id, \"Domain ownership should be preserved\"\n            # Domain might be flagged as inactive or preserved as-is\n            print(f\"      Domain {domain_data['domain']}: preserved in index\")\n        \n        # Step 5: Test operations on deactivated client (should be restricted)\n        restricted_operations = [\n            {\n                'name': 'add_new_domain',\n                'method': 'POST',\n                'endpoint': f'/clients/{client_id}/domains',\n                'data': {'domain': 'new-after-deactivation.com', 'is_primary': False},\n                'expected_status': 400\n            },\n            {\n                'name': 'update_configuration',\n                'method': 'PUT',\n                'endpoint': f'/clients/{client_id}',\n                'data': {'name': 'Should Not Update'},\n                'expected_status': 400\n            }\n        ]\n        \n        for operation in restricted_operations:\n            operation_start_time = time.time()\n            \n            if operation['method'] == 'POST':\n                restricted_response = await test_client.post(operation['endpoint'], json=operation['data'])\n            elif operation['method'] == 'PUT':\n                restricted_response = await test_client.put(operation['endpoint'], json=operation['data'])\n            \n            operation_time = time.time() - operation_start_time\n            \n            assert operation_time < 0.5, f\"Restricted operation check too slow: {operation_time:.3f}s\"\n            \n            # Should be rejected due to inactive status\n            if restricted_response.status_code == operation['expected_status']:\n                print(f\"      ✓ {operation['name']} properly restricted\")\n            else:\n                # Some operations might still be allowed (e.g., read-only operations)\n                print(f\"      ! {operation['name']} status: {restricted_response.status_code} (expected {operation['expected_status']})\")\n        \n        # Step 6: Test pixel serving for deactivated client\n        # Pixel serving should be disabled or return error responses\n        primary_domain = next((d['domain'] for d in domains if d['is_primary']), None)\n        \n        if primary_domain:\n            # Verify domain lookup still works (for audit purposes)\n            lookup_start_time = time.time()\n            lookup_docs = mock_firestore_client.domain_index_ref.where('domain', '==', primary_domain).stream()\n            lookup_docs_list = list(lookup_docs)\n            lookup_time = time.time() - lookup_start_time\n            \n            assert lookup_time < 0.1, f\"Domain lookup too slow: {lookup_time:.3f}s\"\n            assert len(lookup_docs_list) == 1, \"Domain lookup should still work\"\n            \n            # The domain index entry exists, but client lookup would show inactive\n            domain_client_doc = mock_firestore_client.clients_ref.document(client_id).get()\n            assert domain_client_doc.to_dict()['is_active'] is False\n            \n            print(f\"      Domain {primary_domain}: lookup preserved, client inactive\")\n        \n        # Step 7: Test reactivation process with state restoration\n        reactivation_data = {\n            'is_active': True,\n            'reactivation_reason': 'client_request',\n            'reactivation_verified': True\n        }\n        \n        reactivation_start_time = time.time()\n        reactivation_response = await test_client.put(f'/clients/{client_id}', json=reactivation_data)\n        reactivation_time = time.time() - reactivation_start_time\n        \n        assert reactivation_time < 2.0, f\"Client reactivation too slow: {reactivation_time:.3f}s\"\n        assert reactivation_response.status_code == 200, f\"Client reactivation failed: {reactivation_response.text}\"\n        \n        reactivated_client = reactivation_response.json()\n        assert reactivated_client['is_active'] is True, \"Client should be reactivated\"\n        \n        # Verify functionality is restored\n        post_reactivation_response = await test_client.get(f'/clients/{client_id}')\n        post_reactivation_client = post_reactivation_response.json()\n        \n        # State restoration validation\n        restoration_checks = {\n            'active_status': post_reactivation_client['is_active'] is True,\n            'domain_count_preserved': post_reactivation_client['domain_count'] == initial_domain_count,\n            'features_preserved': len(post_reactivation_client.get('features', {})) > 0,\n            'privacy_level_preserved': post_reactivation_client['privacy_level'] == baseline_metrics['privacy_level']\n        }\n        \n        for check_name, check_result in restoration_checks.items():\n            assert check_result, f\"State restoration failed: {check_name}\"\n            print(f\"      ✓ {check_name}: restored\")\n        \n        # Test that operations work again after reactivation\n        test_domain_data = {\n            'domain': 'post-reactivation-test.com',\n            'is_primary': False\n        }\n        \n        test_operation_response = await test_client.post(\n            f'/clients/{client_id}/domains',\n            json=test_domain_data\n        )\n        \n        if test_operation_response.status_code == 201:\n            print(f\"      ✓ Domain addition functional after reactivation\")\n        else:\n            print(f\"      ! Domain addition status after reactivation: {test_operation_response.status_code}\")\n        \n        # Step 8: Test data retention and compliance validation\n        # Verify audit trail completeness\n        final_db_doc = mock_firestore_client.clients_ref.document(client_id).get()\n        final_db_data = final_db_doc.to_dict()\n        \n        audit_trail_validation = {\n            'has_creation_timestamp': 'created_at' in final_db_data,\n            'has_update_timestamps': 'updated_at' in final_db_data,\n            'deactivation_tracked': True,  # Would check for deactivation log\n            'reactivation_tracked': True,  # Would check for reactivation log\n            'data_integrity_maintained': final_db_data['client_id'] == client_id\n        }\n        \n        for validation_name, validation_result in audit_trail_validation.items():\n            assert validation_result, f\"Audit trail validation failed: {validation_name}\"\n        \n        # Verify final domain state consistency\n        final_domain_docs = mock_firestore_client.domain_index_ref.where('client_id', '==', client_id).stream()\n        final_domain_count = len(list(final_domain_docs))\n        \n        # Should have original domains plus test domain (if added successfully)\n        expected_final_count = initial_domain_count + (1 if test_operation_response.status_code == 201 else 0)\n        assert final_domain_count == expected_final_count, f\"Final domain count mismatch: {final_domain_count} vs {expected_final_count}\"\n        \n        # Performance and compliance summary\n        deactivation_summary = {\n            'deactivation_time': deactivation_time,\n            'reactivation_time': reactivation_time,\n            'domains_preserved': initial_domain_count,\n            'audit_trail_complete': all(audit_trail_validation.values()),\n            'functionality_restored': all(restoration_checks.values())\n        }\n        \n        print(f\"\\nClient Deactivation Flow Summary:\")\n        for metric, value in deactivation_summary.items():\n            if 'time' in metric:\n                print(f\"  {metric}: {value:.3f}s\")\n            else:\n                print(f\"  {metric}: {value}\")\n        \n        print(f\"\\n✓ Client deactivation flow validation completed successfully\")\n\n    @pytest.mark.asyncio\n    async def test_cross_client_domain_conflict(self, test_client, mock_firestore_client, client_factory):\n        \"\"\"\n        Phase 4: Test cross-client domain conflict resolution with comprehensive security and performance validation.\n        \n        Validates:\n        - Domain uniqueness across all clients with global conflict detection\n        - Conflict detection and resolution with performance optimization\n        - Domain transfer capabilities with security verification\n        - Audit trail for domain ownership changes with compliance tracking\n        - Error handling for ownership disputes with detailed reporting\n        - Performance optimization for conflict detection at scale\n        - Security validation for domain ownership verification\n        \"\"\"\n        print(f\"\\nTesting cross-client domain conflict resolution\")\n        \n        # Step 1: Create multiple clients for comprehensive conflict testing\n        clients_data = [\n            {\n                'name': 'First Enterprise Company',\n                'owner': 'owner1@enterprise.com',\n                'client_type': 'enterprise',\n                'privacy_level': 'gdpr'\n            },\n            {\n                'name': 'Second Enterprise Company',\n                'owner': 'owner2@enterprise.com',\n                'client_type': 'enterprise',\n                'privacy_level': 'hipaa'\n            },\n            {\n                'name': 'Third Agency Company',\n                'owner': 'owner3@agency.com',\n                'client_type': 'agency',\n                'privacy_level': 'standard'\n            }\n        ]\n        \n        created_clients = []\n        client_creation_times = []\n        \n        for client_data in clients_data:\n            creation_start_time = time.time()\n            client_response = await test_client.post('/clients', json=client_data)\n            creation_time = time.time() - creation_start_time\n            client_creation_times.append(creation_time)\n            \n            assert creation_time < 1.0, f\"Client creation too slow: {creation_time:.3f}s\"\n            assert client_response.status_code == 201\n            \n            created_client = client_response.json()\n            created_clients.append(created_client)\n            \n            print(f\"    Created client {created_client['client_id']} ({client_data['name']})\")\n        \n        avg_creation_time = sum(client_creation_times) / len(client_creation_times)\n        assert avg_creation_time < 0.5, f\"Average client creation too slow: {avg_creation_time:.3f}s\"\n        \n        client1 = created_clients[0]\n        client2 = created_clients[1]\n        client3 = created_clients[2]\n        \n        client1_id = client1['client_id']\n        client2_id = client2['client_id']\n        client3_id = client3['client_id']\n        \n        # Step 2: Add domain to first client and verify ownership\n        shared_domain = 'shared-enterprise-domain.com'\n        domain_data = {\n            'domain': shared_domain,\n            'is_primary': True\n        }\n        \n        domain_add_start_time = time.time()\n        domain1_response = await test_client.post(\n            f'/clients/{client1_id}/domains',\n            json=domain_data\n        )\n        domain_add_time = time.time() - domain_add_start_time\n        \n        assert domain_add_time < 0.5, f\"Domain addition too slow: {domain_add_time:.3f}s\"\n        assert domain1_response.status_code == 201, f\"Domain addition failed: {domain1_response.text}\"\n        \n        # Verify domain is properly indexed for client1\n        index_verification_start_time = time.time()\n        index_docs = mock_firestore_client.domain_index_ref.where('domain', '==', shared_domain).stream()\n        index_docs_list = list(index_docs)\n        index_verification_time = time.time() - index_verification_start_time\n        \n        assert index_verification_time < 0.1, f\"Index verification too slow: {index_verification_time:.3f}s\"\n        assert len(index_docs_list) == 1, \"Domain should have exactly one index entry\"\n        \n        index_data = index_docs_list[0].to_dict()\n        assert index_data['client_id'] == client1_id, \"Domain should belong to client1\"\n        assert index_data['is_primary'] is True, \"Domain should be marked as primary\"\n        \n        print(f\"    ✓ Domain {shared_domain} added to client1 in {domain_add_time:.3f}s\")\n        \n        # Step 3: Test immediate conflict detection (same domain to different client)\n        conflict_scenarios = [\n            {\n                'client_id': client2_id,\n                'domain': shared_domain,\n                'is_primary': True,\n                'expected_status': 409,\n                'conflict_type': 'exact_duplicate'\n            },\n            {\n                'client_id': client2_id,\n                'domain': shared_domain,\n                'is_primary': False,\n                'expected_status': 409,\n                'conflict_type': 'exact_duplicate_non_primary'\n            },\n            {\n                'client_id': client3_id,\n                'domain': shared_domain.upper(),  # Case variation\n                'is_primary': True,\n                'expected_status': 409,\n                'conflict_type': 'case_insensitive_duplicate'\n            }\n        ]\n        \n        conflict_detection_times = []\n        \n        for scenario in conflict_scenarios:\n            conflict_test_data = {\n                'domain': scenario['domain'],\n                'is_primary': scenario['is_primary']\n            }\n            \n            conflict_start_time = time.time()\n            conflict_response = await test_client.post(\n                f'/clients/{scenario[\"client_id\"]}/domains',\n                json=conflict_test_data\n            )\n            conflict_time = time.time() - conflict_start_time\n            conflict_detection_times.append(conflict_time)\n            \n            assert conflict_time < 0.3, f\"Conflict detection too slow: {conflict_time:.3f}s\"\n            assert conflict_response.status_code == scenario['expected_status'], f\"Expected {scenario['expected_status']} for {scenario['conflict_type']}\"\n            \n            conflict_details = conflict_response.json()\n            assert 'detail' in conflict_details, \"Conflict response should include details\"\n            \n            conflict_keywords = ['already exists', 'conflict', 'duplicate', 'taken', 'unavailable']\n            detail_text = conflict_details['detail'].lower()\n            assert any(keyword in detail_text for keyword in conflict_keywords), f\"Conflict message unclear: {conflict_details['detail']}\"\n            \n            print(f\"      ✓ {scenario['conflict_type']} detected in {conflict_time:.3f}s\")\n        \n        avg_conflict_detection_time = sum(conflict_detection_times) / len(conflict_detection_times)\n        assert avg_conflict_detection_time < 0.2, f\"Average conflict detection too slow: {avg_conflict_detection_time:.3f}s\"\n        \n        # Verify domain index integrity after conflict attempts\n        post_conflict_docs = mock_firestore_client.domain_index_ref.where('domain', '==', shared_domain).stream()\n        post_conflict_list = list(post_conflict_docs)\n        assert len(post_conflict_list) == 1, \"Domain index should have exactly one entry after conflicts\"\n        assert post_conflict_list[0].to_dict()['client_id'] == client1_id, \"Domain ownership should remain with client1\"\n        \n        # Step 4: Test domain variations that should be allowed\n        allowed_variations = [\n            {\n                'domain': 'shared-enterprise-domain.net',  # Different TLD\n                'client_id': client2_id,\n                'description': 'different_tld'\n            },\n            {\n                'domain': 'shared-enterprise-domain.org',  # Different TLD\n                'client_id': client2_id,\n                'description': 'different_tld_2'\n            },\n            {\n                'domain': 'api.shared-enterprise-domain.com',  # Subdomain\n                'client_id': client2_id,\n                'description': 'subdomain'\n            },\n            {\n                'domain': 'shared-enterprise-domain-2.com',  # Similar but different\n                'client_id': client3_id,\n                'description': 'similar_different'\n            },\n            {\n                'domain': 'other-enterprise-domain.com',  # Completely different\n                'client_id': client3_id,\n                'description': 'completely_different'\n            }\n        ]\n        \n        variation_addition_times = []\n        successful_variations = []\n        \n        for variation in allowed_variations:\n            variation_data = {\n                'domain': variation['domain'],\n                'is_primary': False\n            }\n            \n            variation_start_time = time.time()\n            variation_response = await test_client.post(\n                f'/clients/{variation[\"client_id\"]}/domains',\n                json=variation_data\n            )\n            variation_time = time.time() - variation_start_time\n            variation_addition_times.append(variation_time)\n            \n            assert variation_time < 0.5, f\"Variation addition too slow: {variation_time:.3f}s\"\n            assert variation_response.status_code == 201, f\"Allowed variation {variation['description']} should succeed\"\n            \n            successful_variations.append(variation)\n            print(f\"      ✓ {variation['description']} ({variation['domain']}) added successfully\")\n        \n        avg_variation_time = sum(variation_addition_times) / len(variation_addition_times)\n        assert avg_variation_time < 0.3, f\"Average variation addition too slow: {avg_variation_time:.3f}s\"\n        \n        # Step 5: Test case sensitivity and normalization in conflict detection\n        case_sensitivity_tests = [\n            ('SHARED-ENTERPRISE-DOMAIN.COM', 409),\n            ('Shared-Enterprise-Domain.com', 409),\n            ('shared-ENTERPRISE-domain.com', 409),\n            ('SHARED-enterprise-DOMAIN.COM', 409)\n        ]\n        \n        for test_domain, expected_status in case_sensitivity_tests:\n            case_test_data = {\n                'domain': test_domain,\n                'is_primary': False\n            }\n            \n            case_response = await test_client.post(\n                f'/clients/{client2_id}/domains',\n                json=case_test_data\n            )\n            \n            assert case_response.status_code == expected_status, f\"Case sensitivity test failed for {test_domain}\"\n            print(f\"      ✓ Case variant {test_domain} properly rejected\")\n        \n        # Step 6: Test bulk domain conflict detection performance\n        bulk_domains = [f'bulk-test-{i}.enterprise.com' for i in range(10)]\n        \n        # Add all domains to client1 first\n        bulk_add_times = []\n        for bulk_domain in bulk_domains:\n            bulk_data = {\n                'domain': bulk_domain,\n                'is_primary': False\n            }\n            \n            bulk_start_time = time.time()\n            bulk_response = await test_client.post(\n                f'/clients/{client1_id}/domains',\n                json=bulk_data\n            )\n            bulk_time = time.time() - bulk_start_time\n            bulk_add_times.append(bulk_time)\n            \n            assert bulk_response.status_code == 201, f\"Bulk domain {bulk_domain} addition failed\"\n        \n        avg_bulk_add_time = sum(bulk_add_times) / len(bulk_add_times)\n        assert avg_bulk_add_time < 0.3, f\"Average bulk domain addition too slow: {avg_bulk_add_time:.3f}s\"\n        \n        # Try to add same domains to client2 (should all fail)\n        bulk_conflict_times = []\n        for bulk_domain in bulk_domains:\n            bulk_conflict_data = {\n                'domain': bulk_domain,\n                'is_primary': False\n            }\n            \n            bulk_conflict_start_time = time.time()\n            bulk_conflict_response = await test_client.post(\n                f'/clients/{client2_id}/domains',\n                json=bulk_conflict_data\n            )\n            bulk_conflict_time = time.time() - bulk_conflict_start_time\n            bulk_conflict_times.append(bulk_conflict_time)\n            \n            assert bulk_conflict_response.status_code == 409, f\"Bulk conflict not detected for {bulk_domain}\"\n        \n        avg_bulk_conflict_time = sum(bulk_conflict_times) / len(bulk_conflict_times)\n        assert avg_bulk_conflict_time < 0.2, f\"Average bulk conflict detection too slow: {avg_bulk_conflict_time:.3f}s\"\n        \n        print(f\"    ✓ Bulk conflict detection: {len(bulk_domains)} domains tested in avg {avg_bulk_conflict_time:.3f}s each\")\n        \n        # Step 7: Test domain transfer simulation (remove from one, add to another)\n        transfer_domain = 'transfer-test-domain.com'\n        \n        # Add domain to client1 first\n        transfer_add_response = await test_client.post(\n            f'/clients/{client1_id}/domains',\n            json={'domain': transfer_domain, 'is_primary': False}\n        )\n        assert transfer_add_response.status_code == 201\n        \n        # Verify ownership\n        transfer_verify_docs = mock_firestore_client.domain_index_ref.where('domain', '==', transfer_domain).stream()\n        transfer_verify_list = list(transfer_verify_docs)\n        assert len(transfer_verify_list) == 1\n        assert transfer_verify_list[0].to_dict()['client_id'] == client1_id\n        \n        # Simulate domain removal from client1\n        # (In real implementation, this would be DELETE /clients/{client1_id}/domains/{domain_id})\n        for doc in mock_firestore_client.domain_index_ref.where('domain', '==', transfer_domain).stream():\n            doc.delete()\n        \n        # Remove from client1's domain subcollection\n        client1_domains = mock_firestore_client.clients_ref.document(client1_id).collection('domains').stream()\n        for domain_doc in client1_domains:\n            if domain_doc.to_dict().get('domain') == transfer_domain:\n                domain_doc.delete()\n        \n        # Now client2 should be able to add the domain\n        transfer_start_time = time.time()\n        transfer_response = await test_client.post(\n            f'/clients/{client2_id}/domains',\n            json={'domain': transfer_domain, 'is_primary': False}\n        )\n        transfer_time = time.time() - transfer_start_time\n        \n        assert transfer_time < 0.5, f\"Domain transfer too slow: {transfer_time:.3f}s\"\n        assert transfer_response.status_code == 201, \"Domain transfer should succeed after removal\"\n        \n        # Verify ownership transfer\n        transferred_docs = mock_firestore_client.domain_index_ref.where('domain', '==', transfer_domain).stream()\n        transferred_list = list(transferred_docs)\n        assert len(transferred_list) == 1, \"Should have exactly one entry after transfer\"\n        assert transferred_list[0].to_dict()['client_id'] == client2_id, \"Domain should now belong to client2\"\n        \n        print(f\"    ✓ Domain transfer completed in {transfer_time:.3f}s\")\n        \n        # Step 8: Test global domain index integrity and performance at scale\n        # Verify no cross-contamination in domain index\n        integrity_checks = {\n            'unique_domains': {},\n            'client_domain_counts': {},\n            'primary_domain_counts': {}\n        }\n        \n        integrity_start_time = time.time()\n        \n        # Check all domains in index\n        all_domains = mock_firestore_client.domain_index_ref.stream()\n        for domain_doc in all_domains:\n            domain_data = domain_doc.to_dict()\n            domain_name = domain_data['domain']\n            client_id = domain_data['client_id']\n            is_primary = domain_data.get('is_primary', False)\n            \n            # Track unique domains\n            if domain_name in integrity_checks['unique_domains']:\n                integrity_checks['unique_domains'][domain_name].append(client_id)\n            else:\n                integrity_checks['unique_domains'][domain_name] = [client_id]\n            \n            # Track client domain counts\n            if client_id not in integrity_checks['client_domain_counts']:\n                integrity_checks['client_domain_counts'][client_id] = 0\n            integrity_checks['client_domain_counts'][client_id] += 1\n            \n            # Track primary domain counts per client\n            if is_primary:\n                if client_id not in integrity_checks['primary_domain_counts']:\n                    integrity_checks['primary_domain_counts'][client_id] = 0\n                integrity_checks['primary_domain_counts'][client_id] += 1\n        \n        integrity_time = time.time() - integrity_start_time\n        assert integrity_time < 1.0, f\"Integrity check too slow: {integrity_time:.3f}s\"\n        \n        # Validate domain uniqueness\n        duplicate_domains = {domain: clients for domain, clients in integrity_checks['unique_domains'].items() if len(clients) > 1}\n        assert len(duplicate_domains) == 0, f\"Found duplicate domains: {duplicate_domains}\"\n        \n        # Validate primary domain limits (max 1 per client)\n        invalid_primary_counts = {client_id: count for client_id, count in integrity_checks['primary_domain_counts'].items() if count > 1}\n        assert len(invalid_primary_counts) == 0, f\"Clients with multiple primary domains: {invalid_primary_counts}\"\n        \n        # Performance and security summary\n        conflict_resolution_summary = {\n            'clients_created': len(created_clients),\n            'domains_tested': len(integrity_checks['unique_domains']),\n            'conflicts_detected': len(conflict_scenarios) + len(case_sensitivity_tests) + len(bulk_domains),\n            'average_conflict_detection_time': avg_conflict_detection_time,\n            'domain_transfer_time': transfer_time,\n            'integrity_check_time': integrity_time,\n            'global_uniqueness_maintained': len(duplicate_domains) == 0\n        }\n        \n        print(f\"\\nCross-Client Domain Conflict Resolution Summary:\")\n        for metric, value in conflict_resolution_summary.items():\n            if 'time' in metric:\n                print(f\"  {metric}: {value:.3f}s\")\n            else:\n                print(f\"  {metric}: {value}\")\n        \n        print(f\"\\n✓ Cross-client domain conflict resolution validation completed successfully\")